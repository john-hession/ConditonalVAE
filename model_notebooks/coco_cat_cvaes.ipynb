{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Deep Learning \n",
    "## Professor Vahid Tarokh\n",
    "### Student: Ashley, John, Ryan, Julian\n",
    "#### Team Project\n",
    "#### Concatenation C-VAE on  COCO, dimensions 64x64, 128x128, 256x256\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### Disclaimer: ChatGPT was used for creating the solution to the project assignment.\n",
    "##### Disclaimer: Solution partly based on HW5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ze1qdIAQuAZ",
    "outputId": "07b0fe37-4e94-4325-e1b9-fdc406a3b1db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ftfy\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: ftfy\n",
      "Successfully installed ftfy-6.3.1\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-7ok2avxd\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-7ok2avxd\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.3.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (24.2)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.6)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.20.1+cu121)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=269238ba36642b72d9292d5242c7640efeacac37ce0a65b76c8b9eb8f83c8fb3\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1kr6n448/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n",
      "Collecting torchmetrics\n",
      "  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.26.4)\n",
      "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.5.1+cu121)\n",
      "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
      "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.16.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
      "Downloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
      "Installing collected packages: lightning-utilities, torchmetrics\n",
      "Successfully installed lightning-utilities-0.11.9 torchmetrics-1.6.0\n",
      "Collecting torch-fidelity\n",
      "  Downloading torch_fidelity-0.3.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-fidelity) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from torch-fidelity) (11.0.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-fidelity) (1.13.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torch-fidelity) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from torch-fidelity) (0.20.1+cu121)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-fidelity) (4.66.6)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torch-fidelity) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torch-fidelity) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torch-fidelity) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torch-fidelity) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torch-fidelity) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->torch-fidelity) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->torch-fidelity) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torch-fidelity) (3.0.2)\n",
      "Downloading torch_fidelity-0.3.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: torch-fidelity\n",
      "Successfully installed torch-fidelity-0.3.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! pip install ftfy regex tqdm\n",
    "! pip install git+https://github.com/openai/CLIP.git\n",
    "! pip install --upgrade torchmetrics\n",
    "! pip install torch-fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bgk38dNNH0i"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pycocotools.coco import COCO\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import clip\n",
    "from typing import List, Union\n",
    "import random\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import CocoCaptions\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from torchmetrics.image import StructuralSimilarityIndexMeasure\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28s2EEsP5JxA",
    "outputId": "c27a01ce-4fa2-4a2f-ce17-de3358c48b90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 138MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# load clip\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "try:\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)  # Set jit=False for better stability\n",
    "    print(\"CLIP loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CLIP: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31mNq79XGqod"
   },
   "source": [
    "## Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UpDykEeEGskn"
   },
   "outputs": [],
   "source": [
    "def generate_from_text(model, text_prompt, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode text with CLIP\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text_prompt)\n",
    "        text_embedding = model.encode_condition(sentences)\n",
    "        text_embedding = text_embedding.to(device)\n",
    "\n",
    "        # Sample from latent space\n",
    "        z = torch.randn(1, model.latent_dim).to(device)\n",
    "\n",
    "        # Generate image\n",
    "        generated_img = model.decode(z, text_embedding)\n",
    "\n",
    "        # Convert to displayable format\n",
    "        generated_img = generated_img.squeeze(0).cpu().permute(1, 2, 0)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(5, 10))\n",
    "        plt.imshow(generated_img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Generated image for: \"{text_prompt}\"')\n",
    "        plt.show()\n",
    "\n",
    "        return generated_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGzk_wwGXtdp"
   },
   "source": [
    "## Dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1BrtF0PNH0i",
    "outputId": "998b5539-40ab-41bf-b7ec-07b7af2c0c26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading annotations...\n",
      "loading annotations into memory...\n",
      "Done (t=1.36s)\n",
      "creating index...\n",
      "index created!\n",
      "Downloading 5000 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [35:27<00:00,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def download_coco_subset(num_images=1000):\n",
    "    os.makedirs('coco_images', exist_ok=True)\n",
    "    os.makedirs('coco_annotations', exist_ok=True)\n",
    "\n",
    "    annotation_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "    if not os.path.exists('coco_annotations/captions_train2017.json'):\n",
    "        print(\"Downloading annotations...\")\n",
    "        response = requests.get(annotation_url)\n",
    "        with open('annotations.zip', 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        with zipfile.ZipFile('annotations.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall('coco_annotations')\n",
    "\n",
    "    coco = COCO('/content/coco_annotations/annotations/captions_train2017.json')\n",
    "\n",
    "    img_ids = coco.getImgIds()\n",
    "    selected_ids = random.sample(img_ids, num_images)\n",
    "\n",
    "    print(f\"Downloading {num_images} images...\")\n",
    "    for img_id in tqdm(selected_ids):\n",
    "        # Get image info\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_url = img_info['coco_url']\n",
    "        file_name = img_info['file_name']\n",
    "        file_path = os.path.join('coco_images', file_name)\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(img_url)\n",
    "            if response.status_code == 200:\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {file_name}: {e}\")\n",
    "\n",
    "    print(\"Download complete!\")\n",
    "    return 'coco_images', 'coco_annotations/captions_train2017.json'\n",
    "\n",
    "image_dir, annotation_file = download_coco_subset(num_images=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SlfFoDeJNH0j",
    "outputId": "281824de-c927-4f77-d24a-9874473e0939"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading annotations...\n",
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "Downloading 1000 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [07:01<00:00,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def download_coco_val_subset(num_images=1000):\n",
    "    os.makedirs('coco_images', exist_ok=True)\n",
    "    os.makedirs('coco_annotations', exist_ok=True)\n",
    "\n",
    "    annotation_url = \"http://images.cocodataset.org/annotations/annotations_trainval2017.zip\"\n",
    "    if not os.path.exists('coco_annotations/captions_train2017.json'):\n",
    "        print(\"Downloading annotations...\")\n",
    "        response = requests.get(annotation_url)\n",
    "        with open('annotations.zip', 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        with zipfile.ZipFile('annotations.zip', 'r') as zip_ref:\n",
    "            zip_ref.extractall('coco_annotations')\n",
    "\n",
    "    coco = COCO('/content/coco_annotations/annotations/captions_val2017.json')\n",
    "\n",
    "    img_ids = coco.getImgIds()\n",
    "    selected_ids = random.sample(img_ids, num_images)\n",
    "\n",
    "    print(f\"Downloading {num_images} images...\")\n",
    "    for img_id in tqdm(selected_ids):\n",
    "        img_info = coco.loadImgs(img_id)[0]\n",
    "        img_url = img_info['coco_url']\n",
    "        file_name = img_info['file_name']\n",
    "        file_path = os.path.join('coco_images', file_name)\n",
    "\n",
    "        if os.path.exists(file_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            response = requests.get(img_url)\n",
    "            if response.status_code == 200:\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {file_name}: {e}\")\n",
    "\n",
    "    print(\"Download complete!\")\n",
    "    return 'coco_images', 'coco_annotations/captions_val2017.json'\n",
    "\n",
    "image_dir_val, annotation_file_val = download_coco_val_subset(num_images=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6s0I81CjNH0j"
   },
   "outputs": [],
   "source": [
    "class COCODatasetWithClip(Dataset):\n",
    "    def __init__(self, root_dir, annotation_file, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.coco = COCO(annotation_file)\n",
    "\n",
    "        all_ids = list(self.coco.imgs.keys())\n",
    "        self.ids = []\n",
    "        for img_id in all_ids:\n",
    "            img_info = self.coco.loadImgs(img_id)[0]\n",
    "            file_path = os.path.join(root_dir, img_info['file_name'])\n",
    "            if os.path.exists(file_path):\n",
    "                self.ids.append(img_id)\n",
    "\n",
    "        print(f\"Found {len(self.ids)} images in directory\")\n",
    "\n",
    "        print(\"Loading CLIP...\")\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "        self.clip_model.eval()\n",
    "\n",
    "        print(\"Pre-encoding captions...\")\n",
    "        self.encoded_captions = {}\n",
    "        for img_id in tqdm(self.ids):\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            captions = [ann['caption'] for ann in anns]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                text_inputs = clip.tokenize(captions).to(device)\n",
    "                text_features = self.clip_model.encode_text(text_inputs)\n",
    "                # Convert to float32 before averaging\n",
    "                text_features = text_features.float()\n",
    "                avg_embedding = text_features.mean(dim=0)\n",
    "                self.encoded_captions[img_id] = avg_embedding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        image_path = os.path.join(self.root_dir, img_info['file_name'])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        caption_embedding = self.encoded_captions[img_id].float()\n",
    "\n",
    "        return image, caption_embedding\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor(),\n",
    "#    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G1ZmMMHfNH0j",
    "outputId": "d310a85b-7dc6-43a8-aa49-b67c3d21c747"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=1.38s)\n",
      "creating index...\n",
      "index created!\n",
      "Found 5000 images in directory\n",
      "Loading CLIP...\n",
      "Pre-encoding captions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:57<00:00, 86.91it/s] \n"
     ]
    }
   ],
   "source": [
    "train_dataset = COCODatasetWithClip(\n",
    "    root_dir='coco_images',\n",
    "    annotation_file='/content/coco_annotations/annotations/captions_train2017.json',\n",
    "    transform=transform)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uNDYkcX0NH0k",
    "outputId": "1b01d9cc-a217-4dc7-8c57-64e1ea5fc4fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "Found 1000 images in directory\n",
      "Loading CLIP...\n",
      "Pre-encoding captions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:11<00:00, 87.76it/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = COCODatasetWithClip(\n",
    "    root_dir='coco_images',\n",
    "    annotation_file='/content/coco_annotations/annotations/captions_val2017.json',\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfCMiEhuXroH"
   },
   "source": [
    "## 256x256 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Er8kTT7HQJ7f"
   },
   "outputs": [],
   "source": [
    "class CatcVAELarge(nn.Module):\n",
    "    def __init__(self, latent_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "        self.clip_model.eval()\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 6, stride=4, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 4, stride=2, padding=1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.flatten_size = 128 * 8 * 8\n",
    "\n",
    "        self.condition_processor = nn.Sequential(\n",
    "            nn.Linear(512, self.flatten_size)\n",
    "        )\n",
    "\n",
    "        # Latent space\n",
    "        self.fc_mu = nn.Linear(self.flatten_size, latent_dim)\n",
    "        self.fc_var = nn.Linear(self.flatten_size, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Linear(latent_dim + 512, 8 * 8 * 128)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 128, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ConvTranspose2d(16, 3, 4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode_condition(self, text):\n",
    "        with torch.no_grad():\n",
    "            embeddings = []\n",
    "            for sentence in text:\n",
    "                embeddings.append(self.clip_model.encode_text(clip.tokenize(sentence).to('cuda')).type(torch.float32))\n",
    "            return torch.mean(torch.stack(embeddings), dim=0)\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        x = self.encoder(x)\n",
    "        return self.fc_mu(x), self.fc_var(x)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):\n",
    "        z = torch.cat([z, c], dim=1)\n",
    "        z = self.decoder_input(z)\n",
    "        z = z.view(-1, 128, 8, 8)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, log_var = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z, c), mu, log_var\n",
    "\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "def sample_images(model, prompts, num_samples=1, device='cuda'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_condition(prompts)\n",
    "\n",
    "        z = torch.randn(len(prompts) * num_samples, model.latent_dim).to(device)\n",
    "\n",
    "        text_features = text_features.repeat_interleave(num_samples, dim=0)\n",
    "        print(text_features)\n",
    "        samples = model.decode(z, text_features)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8z3jRIJsXfFe"
   },
   "source": [
    "## 128x128 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BpgOkTE5cyp"
   },
   "outputs": [],
   "source": [
    "class CatcVAEMed(nn.Module):\n",
    "    def __init__(self, latent_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "        self.clip_model.eval()\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, 4, stride=2, padding=1),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "\n",
    "        self.flatten_size = 128 * 8 * 8\n",
    "\n",
    "        self.condition_processor = nn.Sequential(\n",
    "            nn.Linear(512, self.flatten_size)\n",
    "        )\n",
    "\n",
    "        # Latent space\n",
    "        self.fc_mu = nn.Linear(self.flatten_size, latent_dim)\n",
    "        self.fc_var = nn.Linear(self.flatten_size, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_input = nn.Linear(latent_dim + 512, 8 * 8 * 128)\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 3, 4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode_condition(self, text):\n",
    "        with torch.no_grad():\n",
    "            embeddings = []\n",
    "            for sentence in text:\n",
    "                embeddings.append(self.clip_model.encode_text(clip.tokenize(sentence).to('cuda')).type(torch.float32))\n",
    "            return torch.mean(torch.stack(embeddings), dim=0)\n",
    "\n",
    "    def encode(self, x, c):\n",
    "        x = self.encoder(x)\n",
    "        return self.fc_mu(x), self.fc_var(x)\n",
    "\n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z, c):\n",
    "        z = torch.cat([z, c], dim=1)\n",
    "        z = self.decoder_input(z)\n",
    "        z = z.view(-1, 128, 8, 8)\n",
    "        return self.decoder(z)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        mu, log_var = self.encode(x, c)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return self.decode(z, c), mu, log_var\n",
    "\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "def sample_images(model, prompts, num_samples=1, device='cuda'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_condition(prompts)\n",
    "\n",
    "        z = torch.randn(len(prompts) * num_samples, model.latent_dim).to(device)\n",
    "\n",
    "        text_features = text_features.repeat_interleave(num_samples, dim=0)\n",
    "        print(text_features)\n",
    "        samples = model.decode(z, text_features)\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3LVf3vMXbnL"
   },
   "source": [
    "## 64x64 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LeDf9jtzTN2H"
   },
   "outputs": [],
   "source": [
    "class CatCVAESmall(nn.Module):\n",
    "    def __init__(self, text_embedding_dim=512, latent_dim=256, image_channels=3, image_size=64):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text_embedding_dim (int): Dimension of text embeddings.\n",
    "            latent_dim (int): Dimension of latent space.\n",
    "            image_channels (int): Number of channels in the output image.\n",
    "            image_size (int): Size (height and width) of the generated images (assumes square images).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.text_embedding_dim = text_embedding_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.image_channels = image_channels\n",
    "        self.image_size = image_size\n",
    "\n",
    "        self.clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
    "        self.clip_model.eval()\n",
    "        for param in self.clip_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "        #### for 64x64 images\n",
    "        # Encoder:\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(image_channels, 64, kernel_size=4, stride=2, padding=1),  # 64x64 -> 32x32\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 32x32 -> 16x16\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # 16x16 -> 8x8\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        # Add text embedding and map to latent space dimensions\n",
    "        self.add_text_embedding = nn.Linear(256 * (image_size // 8) ** 2 + text_embedding_dim, 1024)\n",
    "        self.mu = nn.Linear(1024, latent_dim)\n",
    "        self.logvar = nn.Linear(1024, latent_dim)\n",
    "\n",
    "        # Decoder:\n",
    "        self.decoder_input = nn.Linear(latent_dim + text_embedding_dim, 256 * (image_size // 8) ** 2)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 8x8 -> 16x16\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # 16x16 -> 32x32\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, image_channels, kernel_size=4, stride=2, padding=1),  # 32x32 -> 64x64\n",
    "            nn.Sigmoid()  # Outputs normalized to [-1, 1]\n",
    "        )\n",
    "\n",
    "    def encode_condition(self, text):\n",
    "        with torch.no_grad():\n",
    "            embeddings = []\n",
    "            for sentence in text:\n",
    "                embeddings.append(self.clip_model.encode_text(clip.tokenize(sentence).to('cuda')).type(torch.float32))\n",
    "            return torch.mean(torch.stack(embeddings), dim=0)\n",
    "\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick: z = mu + sigma * epsilon.\"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "\n",
    "    def encode(self, images, text_embedding):\n",
    "        \"\"\"Encoder forward pass.\"\"\"\n",
    "        # Encode images\n",
    "        image_features = self.encoder(images)\n",
    "\n",
    "        # Combine image features with text embedding for encoder\n",
    "        combined_features = torch.cat([image_features, text_embedding], dim=1)\n",
    "        latent_space = self.add_text_embedding(combined_features)\n",
    "\n",
    "        # Get mean and standard deviation to sample form the latent space\n",
    "        mu = self.mu(latent_space)\n",
    "        logvar = self.logvar(latent_space)\n",
    "\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "    def decode(self, z, text_embedding):\n",
    "        \"\"\"Decoder forward pass.\"\"\"\n",
    "        # Combine latent space with text embedding for decoder\n",
    "        decoder_input = torch.cat([z, text_embedding], dim=1)\n",
    "        decoder_input = self.decoder_input(decoder_input)\n",
    "        batch_size_dynamic = decoder_input.size(0)  # Dynamically get the batch size\n",
    "        decoder_input = decoder_input.view(batch_size_dynamic, 256, self.image_size // 8, self.image_size // 8) # for 64x64\n",
    "        reconstructed_images = self.decoder(decoder_input)\n",
    "        return reconstructed_images\n",
    "\n",
    "\n",
    "    def forward(self, images, text_embedding):\n",
    "        \"\"\"\n",
    "        Forward pass through the cVAE.\n",
    "        Args:\n",
    "            text_embedding (torch.Tensor): Text embeddings of shape (batch_size, text_embedding_dim).\n",
    "            images (torch.Tensor, optional): Ground-truth images of shape (batch_size, image_channels, image_size, image_size).\n",
    "        Returns:\n",
    "            reconstructed_images (torch.Tensor): Generated images.\n",
    "            mu (torch.Tensor): Mean of latent distribution.\n",
    "            logvar (torch.Tensor): Log variance of latent distribution.\n",
    "        \"\"\"\n",
    "\n",
    "        mu, logvar = self.encode(images, text_embedding)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        reconstructed_images = self.decode(z, text_embedding)\n",
    "        return reconstructed_images, mu, logvar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJZ8VFtxXwmv"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q7dPFpLBNH0m",
    "outputId": "9d936ab3-4227-4c6d-986d-da18350ad4c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 20/20 [00:30<00:00,  1.51s/it, batch_loss=1.5e+6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 4275483.6125 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|██████████| 20/20 [00:29<00:00,  1.46s/it, batch_loss=5.3e+8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 Average loss: 28767547.0125 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|██████████| 20/20 [00:29<00:00,  1.46s/it, batch_loss=1.1e+6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 3 Average loss: 2128763.1000 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|██████████| 20/20 [00:29<00:00,  1.50s/it, batch_loss=1.06e+6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 4 Average loss: 1968157.9000 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|██████████| 20/20 [00:28<00:00,  1.45s/it, batch_loss=1.06e+6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 5 Average loss: 1944687.7125 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|██████████| 20/20 [00:29<00:00,  1.46s/it, batch_loss=1.02e+6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 6 Average loss: 1889781.5531 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|██████████| 20/20 [00:29<00:00,  1.48s/it, batch_loss=9.99e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 7 Average loss: 1868094.1719 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|██████████| 20/20 [00:29<00:00,  1.45s/it, batch_loss=1.03e+6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 8 Average loss: 1860837.4344 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|██████████| 20/20 [00:28<00:00,  1.44s/it, batch_loss=1.02e+6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 9 Average loss: 1847713.5031 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|██████████| 20/20 [00:29<00:00,  1.47s/it, batch_loss=9.98e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 10 Average loss: 1833786.0281 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|██████████| 20/20 [00:29<00:00,  1.46s/it, batch_loss=9.8e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 11 Average loss: 1827870.0688 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|██████████| 20/20 [00:29<00:00,  1.46s/it, batch_loss=9.92e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 12 Average loss: 1820021.0094 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|██████████| 20/20 [00:29<00:00,  1.48s/it, batch_loss=9.93e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 13 Average loss: 1815246.1531 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████| 20/20 [00:28<00:00,  1.44s/it, batch_loss=9.98e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 14 Average loss: 1817746.7500 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████| 20/20 [00:28<00:00,  1.44s/it, batch_loss=9.91e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 15 Average loss: 1812603.4563 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|██████████| 20/20 [00:28<00:00,  1.45s/it, batch_loss=9.88e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 16 Average loss: 1807243.2156 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████| 20/20 [00:28<00:00,  1.44s/it, batch_loss=9.73e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 17 Average loss: 1803559.6250 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|██████████| 20/20 [00:29<00:00,  1.47s/it, batch_loss=9.81e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 18 Average loss: 1803987.9000 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████| 20/20 [00:32<00:00,  1.61s/it, batch_loss=9.69e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 19 Average loss: 1796161.0844 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|██████████| 20/20 [00:30<00:00,  1.55s/it, batch_loss=9.78e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 20 Average loss: 1796071.2531 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|██████████| 20/20 [00:31<00:00,  1.56s/it, batch_loss=9.71e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 21 Average loss: 1791819.8344 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|██████████| 20/20 [00:29<00:00,  1.49s/it, batch_loss=9.8e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 22 Average loss: 1785513.8094 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████| 20/20 [00:28<00:00,  1.45s/it, batch_loss=9.59e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 23 Average loss: 1783177.3500 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████| 20/20 [00:29<00:00,  1.46s/it, batch_loss=9.61e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 24 Average loss: 1780144.5281 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████| 20/20 [00:29<00:00,  1.46s/it, batch_loss=9.59e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 25 Average loss: 1778638.8906 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████| 20/20 [00:30<00:00,  1.50s/it, batch_loss=9.72e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 26 Average loss: 1775663.9563 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████| 20/20 [00:28<00:00,  1.45s/it, batch_loss=9.8e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 27 Average loss: 1774180.2281 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|██████████| 20/20 [00:28<00:00,  1.45s/it, batch_loss=9.79e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 28 Average loss: 1770203.3125 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|██████████| 20/20 [00:28<00:00,  1.45s/it, batch_loss=9.6e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 29 Average loss: 1769306.7875 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|██████████| 20/20 [00:28<00:00,  1.44s/it, batch_loss=9.51e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 30 Average loss: 1766178.6125 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████| 20/20 [00:28<00:00,  1.45s/it, batch_loss=9.57e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 31 Average loss: 1768978.0719 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|██████████| 20/20 [00:28<00:00,  1.44s/it, batch_loss=9.53e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 32 Average loss: 1766347.2188 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|██████████| 20/20 [00:28<00:00,  1.43s/it, batch_loss=9.75e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 33 Average loss: 1762905.5938 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|██████████| 20/20 [00:28<00:00,  1.43s/it, batch_loss=9.59e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 34 Average loss: 1762111.7812 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████| 20/20 [00:28<00:00,  1.45s/it, batch_loss=9.64e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 35 Average loss: 1763300.7000 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|██████████| 20/20 [00:29<00:00,  1.46s/it, batch_loss=9.67e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 36 Average loss: 1762293.1500 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████| 20/20 [00:29<00:00,  1.46s/it, batch_loss=9.61e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 37 Average loss: 1759938.7937 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████| 20/20 [00:29<00:00,  1.46s/it, batch_loss=9.47e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 38 Average loss: 1757622.4438 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|██████████| 20/20 [00:29<00:00,  1.46s/it, batch_loss=9.56e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 39 Average loss: 1758525.2219 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|██████████| 20/20 [00:28<00:00,  1.44s/it, batch_loss=9.58e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 40 Average loss: 1756915.1156 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|██████████| 20/20 [00:28<00:00,  1.45s/it, batch_loss=9.51e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 41 Average loss: 1754413.6344 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|██████████| 20/20 [00:29<00:00,  1.45s/it, batch_loss=9.43e+5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 42 Average loss: 1753185.6156 Learning Rate: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50:  15%|█▌        | 3/20 [00:04<00:23,  1.41s/it, batch_loss=1.79e+6]"
     ]
    }
   ],
   "source": [
    "def train_cvae(model, train_loader, num_epochs=100, learning_rate=1e-4, device=\"cuda\"):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    "    )\n",
    "\n",
    "    train_losses = []\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "\n",
    "        pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "\n",
    "        for batch_idx, (images, captions) in enumerate(pbar):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            recon_batch, mu, log_var = model(images, captions)\n",
    "\n",
    "            loss = loss_function(recon_batch, images, mu, log_var)\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({'batch_loss': loss.item()})\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_epoch_loss)\n",
    "\n",
    "        scheduler.step(avg_epoch_loss)\n",
    "\n",
    "        print(f'====> Epoch: {epoch + 1} Average loss: {avg_epoch_loss:.4f} Learning Rate: {learning_rate}')\n",
    "\n",
    "        if avg_epoch_loss < best_loss:\n",
    "            best_loss = avg_epoch_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_loss,\n",
    "            }, 'best_cvae_model.pth')\n",
    "\n",
    "    return train_losses\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CatCVAESmall()\n",
    "# for medium: model = CatcVAEMed()\n",
    "# for large: model = CatcVAELarge()\n",
    "model.to(device)\n",
    "\n",
    "losses = train_cvae(\n",
    "    model=model,\n",
    "    train_loader=train_dataloader,\n",
    "    num_epochs=50,\n",
    "    learning_rate=1e-3,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zR-707sUNH0n"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zO0Jz51HlNCF"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'catcvae_64.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7b82Qs23tYLb"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFwfuaRfXzLu"
   },
   "source": [
    "## Testing Metric and Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhX0FjZxtcrt"
   },
   "outputs": [],
   "source": [
    "def compute_mse(model, data_loader):\n",
    "\n",
    "    model.eval()\n",
    "    mse_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # reconstruct the data using C-VAE\n",
    "            reconstructed_data, _, _ = model(data, target)\n",
    "            # print(reconstructed_data.shape)\n",
    "            # print(data.shape)\n",
    "            # compute the MSE for the batch\n",
    "            batch_size = data.size(0)\n",
    "            batch_mse = F.mse_loss(reconstructed_data, data, reduction='sum')\n",
    "            mse_loss += batch_mse.item()\n",
    "            total_samples += batch_size\n",
    "\n",
    "    # compute and return the average MSE\n",
    "    average_mse = mse_loss / total_samples\n",
    "    return average_mse\n",
    "\n",
    "\n",
    "def compute_average_ssim(model, data_loader, image_size):\n",
    "    model.eval()\n",
    "    total_samples = 0\n",
    "    total_ssim = 0.0\n",
    "\n",
    "    # Initialize the SSIM metric\n",
    "    ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(device)  # Assumes input data is in [0, 1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in data_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            reconstructed_data, _, _ = model(data, labels)\n",
    "\n",
    "            data = data.view(data.size(0), 3, image_size, image_size)\n",
    "            reconstructed_data = reconstructed_data.view(data.size(0), 3, image_size, image_size)\n",
    "\n",
    "            # Compute SSIM for the batch\n",
    "            batch_ssim = ssim(reconstructed_data, data)\n",
    "            total_ssim += batch_ssim.item() * data.size(0)\n",
    "            total_samples += data.size(0)\n",
    "\n",
    "    # Compute average SSIM\n",
    "    average_ssim = total_ssim / total_samples\n",
    "    return average_ssim\n",
    "\n",
    "\n",
    "def compute_fid(model, data_loader, image_size):\n",
    "    model.eval()\n",
    "    fid = FrechetInceptionDistance(feature=2048).to(device)  # Feature layer 2048 corresponds to InceptionV3\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in data_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "            reconstructed_data, _, _ = model(data, labels)\n",
    "\n",
    "            data = data.view(data.size(0), 3, image_size, image_size)\n",
    "            reconstructed_data = reconstructed_data.view(data.size(0), 3, image_size, image_size)\n",
    "\n",
    "            data = (data * 255).clamp(0, 255).to(torch.uint8)\n",
    "            reconstructed_data = (reconstructed_data * 255).clamp(0, 255).to(torch.uint8)\n",
    "\n",
    "\n",
    "            # Add fake and real data to fid object\n",
    "            fid.update(data, real=True)\n",
    "            fid.update(reconstructed_data, real=False)\n",
    "\n",
    "    fid_score = fid.compute()\n",
    "    return fid_score.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlOePnIdKt03"
   },
   "outputs": [],
   "source": [
    "def show_reconstruction(model, val_dataloader, size, device=\"cuda\"):\n",
    "  truth = []\n",
    "  predicted = []\n",
    "  with torch.no_grad():\n",
    "      for data, labels in val_dataloader:\n",
    "          data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "          #  process input\n",
    "          reconstructed_data, _, _ = model(data, labels)\n",
    "\n",
    "          # Reshape to image format for SSIM computation\n",
    "          data = data.view(data.size(0), 3, size, size)  # (batch_size, channels, height, width)\n",
    "          reconstructed_data = reconstructed_data.view(data.size(0), 3, size, size)\n",
    "          truth.append(data)\n",
    "          predicted.append(reconstructed_data)\n",
    "\n",
    "  def show(img1, img2):\n",
    "      npimg1 = img1.cpu().numpy()\n",
    "      npimg2 = img2.cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "      fig, axes = plt.subplots(1,2, figsize=(20, 10))\n",
    "      axes[0].imshow(np.transpose(npimg1, (1, 2, 0)), interpolation='nearest')\n",
    "      axes[1].imshow(np.transpose(npimg2, (1, 2, 0)), interpolation='nearest')\n",
    "\n",
    "  # show reconstruction results\n",
    "  data, caption = next(iter(val_dataloader))\n",
    "  data, caption = data.to(device), caption.to(device)\n",
    "  reconstructed_data, _, _ = model(data, caption)\n",
    "\n",
    "  data = data[:32]\n",
    "  reconstructed_data = reconstructed_data[:32]\n",
    "\n",
    "  show(make_grid(data), make_grid(reconstructed_data))\n",
    "\n",
    "\n",
    "# results from text\n",
    "\n",
    "def generate_from_text(model, text_prompt, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Encode text with CLIP\n",
    "        text_embedding = model.encode_condition([text_prompt])\n",
    "        text_embedding = text_embedding.to(device)\n",
    "\n",
    "        # Sample from latent space\n",
    "        z = torch.randn(1, model.latent_dim).to(device)\n",
    "\n",
    "        # Generate image\n",
    "        generated_img = model.decode(z, text_embedding)\n",
    "\n",
    "        # Convert to displayable format\n",
    "        generated_img = generated_img.squeeze(0).cpu().permute(1, 2, 0)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(3, 10))\n",
    "        plt.imshow(generated_img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Generated image for: \"{text_prompt}\"')\n",
    "        plt.show()\n",
    "\n",
    "        return generated_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EM1Q20aUX4PA"
   },
   "source": [
    "## Testing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SP9nX3mRKXkn"
   },
   "outputs": [],
   "source": [
    "model = CatCVAESmall()\n",
    "model.load_state_dict(torch.load('/content/catcvae_64.pt'))\n",
    "model.to(device)\n",
    "print('Small model loaded')\n",
    "\n",
    "\n",
    "# get test stats for small model\n",
    "test_mse = compute_mse(model, val_dataloader)\n",
    "print(f'Test MSE: {test_mse:.4f}')\n",
    "test_ssim = compute_average_ssim(model, val_dataloader,64)\n",
    "print(f'Test SSIM: {test_ssim:.4f}')\n",
    "test_fid = compute_fid(model, val_dataloader,64)\n",
    "print(f'Test FID: {test_fid:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYk_Yn7HLAyY"
   },
   "outputs": [],
   "source": [
    "# show reconstruction results\n",
    "show_reconstruction(model, val_dataloader, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o_wzCzZOtv3E"
   },
   "outputs": [],
   "source": [
    "# generate images with small model\n",
    "test_prompts = [\n",
    "    \"a dog playing in the park\",\n",
    "    \"a cat sleeping on a couch\",\n",
    "    \"a sunset over the ocean\",\n",
    "    \"a person riding a bicycle\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    generate_from_text(model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYBbsd-iuzv9"
   },
   "outputs": [],
   "source": [
    "# load medium model\n",
    "model = CatcVAEMed()\n",
    "model.load_state_dict(torch.load('/content/catcvae_128.pt'))\n",
    "model.to(device)\n",
    "print('Small model loaded')\n",
    "\n",
    "\n",
    "# get test stats for med model\n",
    "test_mse = compute_mse(model, val_dataloader)\n",
    "print(f'Test MSE: {test_mse:.4f}')\n",
    "test_ssim = compute_average_ssim(model, val_dataloader,128)\n",
    "print(f'Test SSIM: {test_ssim:.4f}')\n",
    "test_fid = compute_fid(model, val_dataloader,128)\n",
    "print(f'Test FID: {test_fid:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMkTprABu7BU"
   },
   "outputs": [],
   "source": [
    "# show reconstruction results\n",
    "show_reconstruction(model, val_dataloader, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "veb7gYRI5AJR"
   },
   "outputs": [],
   "source": [
    "# generate images with med model\n",
    "test_prompts = [\n",
    "    \"a dog playing in the park\",\n",
    "    \"a cat sleeping on a couch\",\n",
    "    \"a sunset over the ocean\",\n",
    "    \"a person riding a bicycle\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    generate_from_text(model, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlSsxHYuLiHl"
   },
   "outputs": [],
   "source": [
    "# load large model\n",
    "model = CatcVAEMed()\n",
    "model.load_state_dict(torch.load('/content/catcvae_256.pt'))\n",
    "model.to(device)\n",
    "print('Small model loaded')\n",
    "\n",
    "\n",
    "# get test stats for med model\n",
    "test_mse = compute_mse(model, val_dataloader)\n",
    "print(f'Test MSE: {test_mse:.4f}')\n",
    "test_ssim = compute_average_ssim(model, val_dataloader,256)\n",
    "print(f'Test SSIM: {test_ssim:.4f}')\n",
    "test_fid = compute_fid(model, val_dataloader,256)\n",
    "print(f'Test FID: {test_fid:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vFv5vafLiBT"
   },
   "outputs": [],
   "source": [
    "# show reconstruction results\n",
    "show_reconstruction(model, val_dataloader, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZHV8TMEFLh3t"
   },
   "outputs": [],
   "source": [
    "# generate images with large model\n",
    "test_prompts = [\n",
    "    \"a dog playing in the park\",\n",
    "    \"a cat sleeping on a couch\",\n",
    "    \"a sunset over the ocean\",\n",
    "    \"a person riding a bicycle\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    generate_from_text(model, prompt)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
